# Kafka Basics

## [ProducerDemo.java](kafka-basics%2Fsrc%2Fmain%2Fjava%2Fio%2Fconduktor%2Fdemos%2Fkafka%2FProducerDemo.java)

* Starting a producer to send messages.
* Setting key and value serialization to String.

## [ProducerDemoWithCallback.java](kafka-basics%2Fsrc%2Fmain%2Fjava%2Fio%2Fconduktor%2Fdemos%2Fkafka%2FProducerDemoWithCallback.java)

* Starting a producer to send messages.
* Setting key and value serialization to String.
* Registering a callback where to get a response from Kafka after a message is sent

## [ProducerDemoKeys.java](kafka-basics%2Fsrc%2Fmain%2Fjava%2Fio%2Fconduktor%2Fdemos%2Fkafka%2FProducerDemoKeys.java)

* Starting a producer to send records (key+message)
* Setting key and value serialization to String.
* Registering a callback where to get a response from Kafka after a record is sent

## [ConsumerDemo.java](kafka-basics%2Fsrc%2Fmain%2Fjava%2Fio%2Fconduktor%2Fdemos%2Fkafka%2FConsumerDemo.java)

* Starting a consumer to read messages from a topic
* Setting the consumer as part of a consumer group
* Setting the reset offset strategy to earliest

## [ConsumerDemoWithShutdown.java](kafka-basics%2Fsrc%2Fmain%2Fjava%2Fio%2Fconduktor%2Fdemos%2Fkafka%2FConsumerDemoWithShutdown.java)

* The same as above with a strategy to gracefully shutdown a consumer

## [ConsumerDemoCooperative.java](kafka-basics%2Fsrc%2Fmain%2Fjava%2Fio%2Fconduktor%2Fdemos%2Fkafka%2FConsumerDemoCooperative.java)

* The same as above with setting the partition rebalance strategy as Cooperative Rebalance

## [Advanced](kafka-basics%2Fsrc%2Fmain%2Fjava%2Fio%2Fconduktor%2Fdemos%2Fkafka%2Fadvanced)

This package contains some interesting advanced examples for using Kafka, full explanation of the examples can be found here https://www.conduktor.io/kafka/advanced-kafka-consumer-with-java/

# Wikimedia Stream to OpenSearch

The goal of this project is to create a **_producer_** which reads data from wikimedia streaming api, then send messages to a kafka topic. Then we will create a **_consumer_** which subscribes to that topic and bulks information to an OpenSearch instance:

Wikimedia streaming api url: https://stream.wikimedia.org/v2/stream/recentchange

## [kafka-producer-wikimedia](kafka-producer-wikimedia)

* Starts a [WikimediaChangesProducer.java](kafka-producer-wikimedia%2Fsrc%2Fmain%2Fjava%2Fio%2Fconduktor%2Fdemos%2Fkafka%2Fwikimedia%2FWikimediaChangesProducer.java) to read events generated by the wikimedia streaming api, and produces records to the topic `wikimedia.recentchange`.
* Sets properties for a better performant producer.
* Declares a [WikimediaChangeHandler.java](kafka-producer-wikimedia%2Fsrc%2Fmain%2Fjava%2Fio%2Fconduktor%2Fdemos%2Fkafka%2Fwikimedia%2FWikimediaChangeHandler.java) which reacts to every event generated by wikimedia.

#### Exploring the producer

``` bash
# attach the kafka docker container
docker exec -u root -it kafka3 /bin/bash

# Create a new topic named wikimedia.recentchange
kafka-topics.sh --bootstrap-server localhost:9092 \
--topic wikimedia.recentchange --create \
--partitions 3

# Start the Wikimedia producer from intellij

# Start a console consumer to read messages from topic wikimedia.recentchange
kafka-console-consumer.sh --bootstrap-server localhost:9092 \
--topic wikimedia.recentchange --from-beginning
```

## [kafka-consumer-opensearch](kafka-consumer-opensearch)

* Starts a [OpenSearchConsumer.java](kafka-consumer-opensearch%2Fsrc%2Fmain%2Fjava%2Fio%2Fconduktor%2Fdemos%2Fkafka%2Fopensearch%2FOpenSearchConsumer.java) which consumes records from the topic `wikimedia.recentchange` and push them all to the index `wikimedia` in OpenSearch.
* The consumer is configured to commit offsets with **at least once** delivery semantic.

### Exploring the consumer

1. Starts the Wikimedia producer from intellij
2. Starts the Wikimedia consumer from intellij
3. Go to OpenSearch Dashboard: http://localhost:5601/app/home#/ and check the index is being populated:

```sql
GET /wikimedia.recentchange/_search
{
  "query": {
    "match_all": {}
  }
}
```

##[kafka-streams-wikimedia](kafka-streams-wikimedia)

Kafka streams application that consumes records from the topic `wikimedia.recentchange` to perform some analytics and produce the results in 3 different topics:

* `wikimedia.stats.bots` -> identifies which events are bots or not.
* `wikimedia.stats.website` -> stats for each website.
* `wikimedia.stats.timeseries` -> how many records gotten per second.


### Exploring the kafka streaming application

1. Create topics
``` bash
# attach the kafka docker container
docker exec -u root -it kafka3 /bin/bash

# Create a new topic named wikimedia.stats.bots
kafka-topics.sh --bootstrap-server localhost:9092 \
--topic wikimedia.stats.bots --create \
--partitions 3

# Create a new topic named wikimedia.stats.website
kafka-topics.sh --bootstrap-server localhost:9092 \
--topic wikimedia.stats.website --create \
--partitions 3

# Create a new topic named wikimedia.stats.timeseries
kafka-topics.sh --bootstrap-server localhost:9092 \
--topic wikimedia.stats.timeseries --create \
--partitions 3
```

1. Starts the Wikimedia producer from intellij
2. Starts the kafka streams app from intellij
3. Go to Offset Explorer and check `wikimedia.stats.bots` `wikimedia.stats.website` and `wikimedia.stats.timeseries`

